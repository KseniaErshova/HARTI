{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries and functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "RANDOM_STATE = 42  # Pseudo-random state\n",
    "\n",
    "from utils import *\n",
    "sns.set_palette(\"tab10\") # Default seaborn theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataset\n",
    "fn_vae_data = glob.glob('./Updated*.pkl')\n",
    "latest_fn_vae_data = max(fn_vae_data, key=os.path.getctime)\n",
    "\n",
    "print(\"Loading... \",latest_fn_vae_data)\n",
    "with open(latest_fn_vae_data, \"rb\") as f:\n",
    "    vae_data_main = pickle.load(f)\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate how many patients had how many cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of patients and number of cases \n",
    "print('Total patients: ', vae_data_main[['ID']].groupby('ID').max().shape[0])\n",
    "print('Total ICU admissions: ', vae_data_main[['ID_subid']].groupby('ID_subid').max().shape[0])\n",
    "\n",
    "# ICU admissions per group\n",
    "vae_data_main[['ID_subid', 'group']].groupby('ID_subid').max().group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patient per group\n",
    "vae_data_main[['ID', 'group']].groupby('ID').max().group.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of cases per one ICU admission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "\n",
    "class AggFuncWFlag():\n",
    "    def __init__(self):\n",
    "        self.flag = False\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        if not self.flag:\n",
    "            self.flag = True\n",
    "            \n",
    "            if x.iloc[0] == 0. and x.iloc[1] == 1:\n",
    "                return 1\n",
    "            \n",
    "            elif x.iloc[0] == 0:\n",
    "                return 0\n",
    "            elif x.iloc[0] == 1.:\n",
    "                return 1\n",
    "            else:\n",
    "                raise ValueError()\n",
    "         \n",
    "        return x.iloc[0] == 0. and x.iloc[1] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation\n",
    "\n",
    "agg_func = lambda x: x.iloc[0] == 0. and x.iloc[1] == 1\n",
    "agg_func = AggFuncWFlag()\n",
    "\n",
    "result = []\n",
    "for COL in ['non_vap_resp_hai', 'vap', 'infection_respiratory']:\n",
    "\n",
    "    res = []\n",
    "    for uid in vae_data_main.loc[vae_data_main[COL] > 0, 'ID_subid'].unique():\n",
    "        agg_func = AggFuncWFlag()\n",
    "        res.append(\n",
    "            vae_data_main.loc[vae_data_main.ID_subid == uid, COL].rolling(2).agg(agg_func).sum())\n",
    "\n",
    "    # Check guys w/ one line only\n",
    "    one_liners = vae_data_main.loc[vae_data_main[COL] > 0, 'ID_subid'].unique()\n",
    "    one_liners_ids = one_liners[[i for (i,j) in enumerate(res) if j == 0]]\n",
    "\n",
    "    assert vae_data_main.loc[(vae_data_main.ID_subid.isin(one_liners_ids) & \n",
    "                              vae_data_main[COL] > 0)].shape[0] == len(one_liners_ids)\n",
    "\n",
    "    # OK\n",
    "    result += [pd.Series(res).replace(0, 1).value_counts()]\n",
    "    \n",
    "pd.concat(result, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.Series(res).replace(0, 1).value_counts()\n",
    "print('Total HARTI cases: ', sum(total.index * total.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The onset of HARTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The onset of HARTI depending on days in the ICU\n",
    "nva = vae_data_main.loc[vae_data_main['group'] == 'NVA-HARTI']\n",
    "nva = nva.loc[:, ['day_in_icu_bid', 'ID_subid']].groupby('ID_subid').max()\n",
    "print('Median day in ICU when NVA-HARTI begins: ', nva.median()[0])\n",
    "\n",
    "vap = vae_data_main.loc[vae_data_main['group'] == 'VA-HARTI']\n",
    "vap = vap.loc[:, ['day_in_icu_bid', 'ID_subid']].groupby('ID_subid').max()\n",
    "print('Median day in ICU when VA-HARTI begins: ', vap.median()[0])\n",
    "\n",
    "fig, (ax, ax1) = plt.subplots(2, figsize=(8,4))\n",
    "fig.suptitle('The onset of HARTI depending on days in the ICU', fontsize=15)\n",
    "colors_sns = ['medium blue']\n",
    "sns.set_palette(sns.xkcd_palette(colors_sns))\n",
    "sns.boxplot(x=vap.day_in_icu_bid, orient='h', ax=ax)\n",
    "ax.grid(linestyle='dotted')\n",
    "ax.set_xlabel('')\n",
    "ax.set_xticklabels([''])\n",
    "ax.set_xlim(0,60)\n",
    "ax.set_ylabel('VA-HARTI')\n",
    "ax.minorticks_on()\n",
    "ax.grid(linestyle='dotted', which='both')\n",
    "\n",
    "colors_sns = ['orange']\n",
    "sns.set_palette(sns.xkcd_palette(colors_sns))\n",
    "sns.boxplot(x=nva.day_in_icu_bid, orient='h', ax=ax1)\n",
    "ax1.minorticks_on()\n",
    "ax1.grid(linestyle='dotted', which='both')\n",
    "ax1.set_xlabel('Days in the ICU before the onset of infection')\n",
    "ax1.set_xlim(0,60)\n",
    "ax1.set_ylabel('NVA-HARTI')\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.savefig('./pictures/onset_all.pdf', dpi=600)\n",
    "\n",
    "# Restore default colors\n",
    "sns.set_palette(\"tab10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The onset of VA-HARTI depending on days on mechanical ventilation\n",
    "vap = vae_data_main.loc[vae_data_main['group'] == 'VA-HARTI']\n",
    "vap = vap.loc[:, ['mech_vent_bid', 'ID_subid']].groupby('ID_subid').max()\n",
    "# Print median days on ventilation before infection\n",
    "print('Median days on ventilation when VA-HARTI begins: ', vap.median()[0])\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(8, 3))\n",
    "sns.boxplot(x=vap.mech_vent_bid, orient='h', ax=ax)\n",
    "ax.set_xlabel('Days on mechanical ventilation before the onset of VA-HARTI')\n",
    "ax.grid(linestyle='dotted')\n",
    "ax.set_xlim(0,60)\n",
    "ax.set_ylabel('VA-HARTI')\n",
    "ax.minorticks_on()\n",
    "ax.grid(linestyle='dotted', which='both')\n",
    "ax.set_title('The onset of VA-HARTI depending on days on mechanical ventilation')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./pictures/onset_va.pdf', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The duration of HARTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data\n",
    "nva = vae_data_main.loc[vae_data_main['group'] == 'NVA-HARTI']\n",
    "nva = nva.loc[:, ['non_vap_resp_hai', 'ID_subid']].groupby('ID_subid').sum()\n",
    "vap = vae_data_main.loc[vae_data_main['group'] == 'VA-HARTI']\n",
    "vap = vap.loc[:, ['vap', 'ID_subid']].groupby('ID_subid').sum()\n",
    "\n",
    "# Print median duration of infections\n",
    "print('Median duration of VA-HARTI: ', vap.median()[0])\n",
    "print('Median duration of NVA-HARTI: ', nva.median()[0])\n",
    "\n",
    "# Plot in boxplot format\n",
    "fig, (ax, ax1) = plt.subplots(2, figsize=(8,4))\n",
    "fig.suptitle('Duration of infection in patients with VA- and NVA-HARTI', fontsize=15)\n",
    "colors_sns = ['medium blue']\n",
    "sns.set_palette(sns.xkcd_palette(colors_sns))\n",
    "sns.boxplot(x=vap.vap, orient='h', ax=ax)\n",
    "ax.set_xlabel('')\n",
    "ax.set_xticklabels([''])\n",
    "ax.grid(linestyle='dotted')\n",
    "ax.set_xlim(0,60)\n",
    "ax.set_ylabel('VA-HARTI')\n",
    "ax.minorticks_on()\n",
    "ax.grid(linestyle='dotted', which='both')\n",
    "\n",
    "colors_sns = ['orange']\n",
    "sns.set_palette(sns.xkcd_palette(colors_sns))\n",
    "sns.boxplot(x=nva.non_vap_resp_hai, orient='h', ax=ax1)\n",
    "ax1.minorticks_on()\n",
    "ax1.grid(linestyle='dotted', which='both')\n",
    "ax1.set_xlabel('Duration of HARTI symptoms, days')\n",
    "ax1.set_xlim(0,60)\n",
    "ax1.set_ylabel('NVA-HARTI')\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.savefig('./pictures/duration_boxplot.pdf', dpi=600)\n",
    "\n",
    "sns.set_palette(\"tab10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare groups VA-, NVA-, No HAI, Other HAI, Dual HARTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select group data\n",
    "data_vap = vae_data_main[(vae_data_main.group=='VA-HARTI')].groupby('ID_subid').max()\n",
    "data_nva = vae_data_main[(vae_data_main.group=='NVA-HARTI')].groupby('ID_subid').max()\n",
    "data_no_hai = vae_data_main[(vae_data_main.group=='No HAI')].groupby('ID_subid').max()\n",
    "data_other_hai = vae_data_main[(vae_data_main.group=='Other HAI')].groupby('ID_subid').max()\n",
    "data_both = vae_data_main[(vae_data_main.group=='Dual HARTI')].groupby('ID_subid').max()\n",
    "\n",
    "### Select factors to compare\n",
    "# Numeric factors\n",
    "FACTORS_numeric = ['age', 'charlson',  # condition on admission\n",
    "           \n",
    "                   'st_all_sum', 'st_craniotomy_len_sum', # surgeries\n",
    "                   'st_device_len_sum', 'st_endonasal_len_sum', 'st_endovascular_len_sum',\n",
    "                   'st_other_len_sum', 'st_spinal_len_sum', 'st_all_len_sum',\n",
    "                   \n",
    "                   'gcs', 'rass', 'pbss',  # severity of patients condition\n",
    "           \n",
    "                   'mech_vent_days',   'antibiotics_total_binary_days',  # ICU care\n",
    "                   'central_line_days', 'feeding_tube_days', 'arterial_line_days', 'evd_days', 'icpm_days',\n",
    "                   'urinary_catheter_days', 'hypothermia_days', 'hemodialysis_days',\n",
    "                   'total_parenteral_feeding_days', 'sedation_days', 'anxiolytics_days',\n",
    "                   'vasopressors_days', 'days_mech_vent_before_tracheostomy', 'days_before_tracheostomy',\n",
    "                   'endotracheal_tube_1_days', 'endotracheal_tube_2_days', 'endotracheal_tube_3_days',\n",
    "                   \n",
    "                   'intestinal_dysfunction_days', 'infection_bloodstream_days', # complications\n",
    "                   'infection_other_days', 'infection_urinary_days', 'infection_cns_days', 'infection_ssi_days'\n",
    "                  ]\n",
    "\n",
    "# Binary factors\n",
    "FACTORS_binary = ['gender_M', 'disease_type_trauma', 'disease_type_tumor', # condition on admission\n",
    "                  'disease_type_vascular', 'disease_type_other',\n",
    "                  \n",
    "                  'mutism', 'convulsions', 'aphasia', 'vegetative_state',  # severity of patients condition\n",
    "                  \n",
    "                  'st_device_count', 'st_other_count', 'st_craniotomy_count', 'st_endovascular_count', # surgeries\n",
    "                  'st_endonasal_count', 'st_spinal_count',\n",
    "                  \n",
    "                  'mech_vent',   'antibiotics_total_binary',  # ICU care\n",
    "                  'central_line', 'feeding_tube', 'arterial_line', 'evd', 'icpm',\n",
    "                  'urinary_catheter', 'hypothermia', 'hemodialysis', 'total_parenteral_feeding',\n",
    "                  'sedation', 'anxiolytics', 'vasopressors',\n",
    "           \n",
    "                  'intestinal_dysfunction', 'infection_bloodstream', 'infection_other',  # complications\n",
    "                  'infection_urinary', 'infection_cns', 'infection_ssi', 'csfl_ne', 'csfl_ss'\n",
    "                 ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = {'vap': data_vap, \n",
    "         'nva': data_nva, \n",
    "         'no_hai': data_no_hai,\n",
    "         'other_hai' : data_other_hai,\n",
    "         'both' : data_both\n",
    "        }\n",
    "# Numeric factors\n",
    "group_dict = {}\n",
    "for key, d in dsets.items():\n",
    "    median = []\n",
    "    q25 = []\n",
    "    q75 = []\n",
    "    for col in FACTORS_numeric:\n",
    "        median.append(d[col].median())\n",
    "        q25.append(np.percentile(d[col].dropna(), 25))\n",
    "        q75.append(np.percentile(d[col].dropna(), 75))\n",
    "    group_dict[key] = pd.DataFrame(list(zip(median, q25, q75)),\n",
    "                       columns = ['Median / n_' + key, 'Q25 / lower_'+key, 'Q75 / upper_'+key],\n",
    "                                   index = FACTORS_numeric)\n",
    "\n",
    "numeric = pd.concat([pd.DataFrame(group_dict['vap']), pd.DataFrame(group_dict['nva']),\n",
    "                    pd.DataFrame(group_dict['no_hai']), pd.DataFrame(group_dict['other_hai']),\n",
    "                    pd.DataFrame(group_dict['both'])], axis=1)\n",
    "\n",
    "# Binary factors\n",
    "group_dict = {}\n",
    "for key, d in dsets.items():\n",
    "    n = []\n",
    "    perc = []\n",
    "    lower = []\n",
    "    upper = []\n",
    "    for col in FACTORS_binary:\n",
    "        n.append((d[col]>0).sum())\n",
    "        perc.append(round(((d[col]>0).sum() / len(d[col])*100), 1))\n",
    "        nobs = len(d[col])\n",
    "        count = (d[col]>0).sum()\n",
    "        left, right = ci(count, nobs)\n",
    "        lower.append(round(left*100, 1))\n",
    "        upper.append(round(right*100, 1))\n",
    "    \n",
    "    group_dict[key] = pd.DataFrame(list(zip(n, perc, lower, upper)),\n",
    "                       columns = ['Median / n_' + key, '%_'+key, 'Q25 / lower_'+key, 'Q75 / upper_'+key],\n",
    "                                   index = FACTORS_binary)   \n",
    "\n",
    "binary = pd.concat([pd.DataFrame(group_dict['vap']), pd.DataFrame(group_dict['nva']),\n",
    "                    pd.DataFrame(group_dict['no_hai']), pd.DataFrame(group_dict['other_hai']),\n",
    "                    pd.DataFrame(group_dict['both'])], axis=1)\n",
    "binary[\"Median / n_vap\"] = binary[\"Median / n_vap\"].astype(int).map(str) +\" (\"+ binary[\"%_vap\"].map(str) + \"%)\"\n",
    "binary[\"Median / n_nva\"] = binary[\"Median / n_nva\"].astype(int).map(str) +\" (\"+ binary[\"%_nva\"].map(str) + \"%)\"\n",
    "binary[\"Median / n_no_hai\"] = binary[\"Median / n_no_hai\"].astype(int).map(str) +\" (\"+ binary[\"%_no_hai\"].map(str) + \"%)\"\n",
    "binary[\"Median / n_other_hai\"] = binary[\"Median / n_other_hai\"].astype(int).map(str) +\" (\"+ binary[\"%_other_hai\"].map(str) + \"%)\"\n",
    "binary[\"Median / n_both\"] = binary[\"Median / n_both\"].astype(int).map(str) +\" (\"+ binary[\"%_both\"].map(str) + \"%)\"\n",
    "binary.drop(['%_vap', '%_nva', '%_no_hai', '%_other_hai', '%_both'], axis=1, inplace=True)\n",
    "\n",
    "#Combine numeric and binary tables\n",
    "compare_all_groups = pd.concat([numeric, binary], axis=0)\n",
    "\n",
    "# Format columns\n",
    "compare_all_groups['Q/CI_vap'] = compare_all_groups[['Q25 / lower_vap','Q75 / upper_vap']].apply(lambda x : '[{}; {}]'.format(x[0],x[1]), axis=1)\n",
    "compare_all_groups['Q/CI_nva'] = compare_all_groups[['Q25 / lower_nva','Q75 / upper_nva']].apply(lambda x : '[{}; {}]'.format(x[0],x[1]), axis=1)\n",
    "compare_all_groups['Q/CI_no_hai'] = compare_all_groups[['Q25 / lower_no_hai','Q75 / upper_no_hai']].apply(lambda x : '[{}; {}]'.format(x[0],x[1]), axis=1)\n",
    "compare_all_groups['Q/CI_other_hai'] = compare_all_groups[['Q25 / lower_other_hai','Q75 / upper_other_hai']].apply(lambda x : '[{}; {}]'.format(x[0],x[1]), axis=1)\n",
    "compare_all_groups['Q/CI_both'] = compare_all_groups[['Q25 / lower_both','Q75 / upper_both']].apply(lambda x : '[{}; {}]'.format(x[0],x[1]), axis=1)\n",
    "\n",
    "compare_all_groups.drop(['Q25 / lower_vap', 'Q75 / upper_vap', 'Q25 / lower_nva',\n",
    "                         'Q75 / upper_nva', 'Q25 / lower_no_hai', 'Q75 / upper_no_hai',\n",
    "                        'Q25 / lower_other_hai', 'Q75 / upper_other_hai', 'Q25 / lower_both',\n",
    "                         'Q75 / upper_both',], axis=1, inplace=True)\n",
    "compare_all_groups = compare_all_groups[['Median / n_nva', 'Q/CI_nva',\n",
    "                                         'Median / n_no_hai', 'Q/CI_no_hai',\n",
    "                                         'Median / n_vap', 'Q/CI_vap',\n",
    "                                         'Median / n_other_hai', 'Q/CI_other_hai',\n",
    "                                         'Median / n_both', 'Q/CI_both',]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hypothesis\n",
    "# Kruskal test for numeric (skip NaN)\n",
    "res = {}\n",
    "for factor in FACTORS_numeric:\n",
    "    res[factor] = kruskal(data_vap[factor].values, data_nva[factor].values,\n",
    "                          data_no_hai[factor].values, data_other_hai[factor].values, data_both[factor].values,\n",
    "                          nan_policy='omit').pvalue\n",
    "\n",
    "if not compare_all_groups.columns.str.contains('pvalue').max():\n",
    "    compare_all_groups = compare_all_groups.join(pd.DataFrame({'pvalue': res}))\n",
    "    \n",
    "# Chi-square for binary\n",
    "res = {}\n",
    "for factor in FACTORS_binary:\n",
    "    contigency = pd.crosstab(vae_data_main[['ID_subid', 'group']].groupby('ID_subid').max()['group'],\n",
    "                            vae_data_main[['ID_subid', factor]].groupby('ID_subid').max()[factor]>0)\n",
    "    pvalue = chi(contigency)[1]\n",
    "    compare_all_groups.loc[factor, 'pvalue'] = pvalue\n",
    "    \n",
    "# Adjust p-value for multiple comparison \n",
    "compare_all_groups['adjusted_pvalue'] = pd.Series(multipletests(compare_all_groups.pvalue.dropna().values)[1],\n",
    "                                       index=compare_all_groups.pvalue.dropna().index)\n",
    "compare_all_groups['adjusted_pvalue'] = compare_all_groups['adjusted_pvalue'].apply(lambda x: round(x, 5))\n",
    "compare_all_groups['pvalue'] = compare_all_groups['pvalue'].apply(lambda x: round(x, 5))\n",
    "\n",
    "# Match column names from json dict\n",
    "f = open(\"./columns_dict.json\")\n",
    "columns_dict = json.load(f)\n",
    "compare_all_groups.index = compare_all_groups.index.to_series().map(columns_dict.get)\n",
    "\n",
    "# Save table\n",
    "compare_all_groups.to_csv('./output_data/compare_all_groups.csv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "compare_all_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare VA-HARTI and NVA-HARTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = {'vap': data_vap, \n",
    "         'nva': data_nva}\n",
    "\n",
    "# Numeric factors\n",
    "group_dict = {}\n",
    "for key, d in dsets.items():\n",
    "    median = []\n",
    "    q25 = []\n",
    "    q75 = []\n",
    "    for col in FACTORS_numeric:\n",
    "        median.append(d[col].median())\n",
    "        q25.append(np.percentile(d[col].dropna(), 25))\n",
    "        q75.append(np.percentile(d[col].dropna(), 75))\n",
    "    group_dict[key] = pd.DataFrame(list(zip(median, q25, q75)),\n",
    "                       columns = ['Median / n_' + key, 'Q25 / lower_'+key, 'Q75 / upper_'+key],\n",
    "                                   index = FACTORS_numeric)\n",
    "\n",
    "numeric = pd.concat([pd.DataFrame(group_dict['vap']), pd.DataFrame(group_dict['nva'])], axis=1)\n",
    "\n",
    "# Binary factors\n",
    "group_dict = {}\n",
    "for key, d in dsets.items():\n",
    "    n = []\n",
    "    perc = []\n",
    "    lower = []\n",
    "    upper = []\n",
    "    for col in FACTORS_binary:\n",
    "        n.append((d[col]>0).sum())\n",
    "        perc.append(round(((d[col]>0).sum() / len(d[col])*100), 1))\n",
    "        nobs = len(d[col])\n",
    "        count = (d[col]>0).sum()\n",
    "        left, right = ci(count, nobs)\n",
    "        lower.append(round(left*100, 1))\n",
    "        upper.append(round(right*100, 1))\n",
    "    \n",
    "    group_dict[key] = pd.DataFrame(list(zip(n, perc, lower, upper)),\n",
    "                       columns = ['Median / n_' + key, '%_'+key, 'Q25 / lower_'+key, 'Q75 / upper_'+key],\n",
    "                                   index = FACTORS_binary)   \n",
    "\n",
    "binary = pd.concat([pd.DataFrame(group_dict['vap']), pd.DataFrame(group_dict['nva'])], axis=1)\n",
    "binary[\"Median / n_vap\"] = binary[\"Median / n_vap\"].astype(int).map(str) +\" (\"+ binary[\"%_vap\"].map(str) + \"%)\"\n",
    "binary[\"Median / n_nva\"] = binary[\"Median / n_nva\"].astype(int).map(str) +\" (\"+ binary[\"%_nva\"].map(str) + \"%)\"\n",
    "binary.drop(['%_vap', '%_nva'], axis=1, inplace=True)\n",
    "\n",
    "#Combine numeric and binary tables\n",
    "compare_harti_groups = pd.concat([numeric, binary], axis=0)\n",
    "\n",
    "# Format columns\n",
    "compare_harti_groups['Q/CI_vap'] = compare_harti_groups[['Q25 / lower_vap','Q75 / upper_vap']].apply(lambda x : '[{}; {}]'.format(x[0],x[1]), axis=1)\n",
    "compare_harti_groups['Q/CI_nva'] = compare_harti_groups[['Q25 / lower_nva','Q75 / upper_nva']].apply(lambda x : '[{}; {}]'.format(x[0],x[1]), axis=1)\n",
    "compare_harti_groups.drop(['Q25 / lower_vap','Q75 / upper_vap', 'Q25 / lower_nva','Q75 / upper_nva'], axis=1, inplace=True)\n",
    "compare_harti_groups = compare_harti_groups[['Median / n_vap', 'Q/CI_vap', 'Median / n_nva', 'Q/CI_nva']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hypothesis\n",
    "# Kruskal test for numeric (skip NaN)\n",
    "res = {}\n",
    "for factor in FACTORS_numeric:\n",
    "    res[factor] = kruskal(data_vap[factor].values,data_nva[factor].values, nan_policy='omit').pvalue\n",
    "\n",
    "if not compare_harti_groups.columns.str.contains('pvalue').max():\n",
    "    compare_harti_groups = compare_harti_groups.join(pd.DataFrame({'pvalue': res}))\n",
    "    \n",
    "# Chi-square for binary\n",
    "res = {}\n",
    "for factor in FACTORS_binary:\n",
    "    contigency = pd.crosstab(vae_data_main[['ID_subid', 'group']].groupby('ID_subid').max()['group'],\n",
    "                            vae_data_main[['ID_subid', factor]].groupby('ID_subid').max()[factor]>0)\n",
    "    contigency = contigency.loc[[\"NVA-HARTI\", \"VA-HARTI\"]]\n",
    "    pvalue = chi(contigency)[1]\n",
    "    compare_harti_groups.loc[factor, 'pvalue'] = pvalue\n",
    "    \n",
    "# Adjust p-value for multiple comparison \n",
    "compare_harti_groups['adjusted_pvalue'] = pd.Series(multipletests(compare_harti_groups.pvalue.dropna().values)[1],\n",
    "                                       index=compare_harti_groups.pvalue.dropna().index)\n",
    "compare_harti_groups['adjusted_pvalue'] = compare_harti_groups['adjusted_pvalue'].apply(lambda x: round(x, 5))\n",
    "compare_harti_groups['pvalue'] = compare_harti_groups['pvalue'].apply(lambda x: round(x, 5))\n",
    "\n",
    "# Match column names from json dict\n",
    "f = open(\"./columns_dict.json\")\n",
    "columns_dict = json.load(f)\n",
    "compare_harti_groups.index = compare_harti_groups.index.to_series().map(columns_dict.get)\n",
    "\n",
    "# Save table\n",
    "compare_harti_groups.to_csv('./output_data/compare_harti_groups.csv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "compare_harti_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Population dynamics in time\n",
    "[Stationarity](https://www.analyticsvidhya.com/blog/2018/09/non-stationary-time-series-python/)\n",
    "[ADF Test](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.adfuller.html)\n",
    "\n",
    "#### Columns that are used to diagnose HARTI - we don't use them:\n",
    "cols_dx = ['tbd_sanation', 'fio2', 'purulent_sputum', 'xray_inf', 'pleural_drain', 'temperature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ICU admissions per year\n",
    "vae_data_main[['ID_subid', 'year']].groupby('ID_subid').max().reset_index().groupby('year').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select patient characteristics and interventions with possible changes over time\n",
    "\n",
    "FACTORS = ['age', 'charlson', # condition on admission\n",
    "           'gender_M', 'disease_type_trauma', 'disease_type_tumor',\n",
    "           'disease_type_vascular', 'disease_type_other',\n",
    "           \n",
    "           'st_all_sum', 'st_craniotomy_len_sum', # surgeries\n",
    "           'st_device_len_sum', 'st_endonasal_len_sum', 'st_endovascular_len_sum',\n",
    "           'st_other_len_sum', 'st_spinal_len_sum', 'st_all_len_sum',\n",
    "           'st_device_count', 'st_other_count', 'st_craniotomy_count', 'st_endovascular_count',\n",
    "           'st_endonasal_count', 'st_spinal_count',\n",
    "           \n",
    "           'gcs', 'rass', 'pbss',  # severity of patients condition\n",
    "           'mutism', 'convulsions', 'aphasia', 'vegetative_state',\n",
    "           \n",
    "           'mech_vent_days',   'antibiotics_total_binary_days',  # ICU care\n",
    "           'central_line_days', 'feeding_tube_days', 'arterial_line_days', 'evd_days', 'icpm_days',\n",
    "           'urinary_catheter_days', 'hypothermia_days', 'hemodialysis_days', 'total_parenteral_feeding_days',\n",
    "           'sedation_days', 'anxiolytics_days', 'vasopressors_days', 'days_mech_vent_before_tracheostomy',\n",
    "           'days_before_tracheostomy', 'endotracheal_tube_1_days', 'endotracheal_tube_2_days',\n",
    "           'endotracheal_tube_3_days', 'mech_vent',   'antibiotics_total_binary',\n",
    "           'central_line', 'feeding_tube', 'arterial_line', 'evd', 'icpm',\n",
    "           'urinary_catheter', 'hypothermia', 'hemodialysis', 'total_parenteral_feeding',\n",
    "           'sedation', 'anxiolytics', 'vasopressors',\n",
    "            \n",
    "           'intestinal_dysfunction', 'infection_bloodstream', 'infection_other',  # complications\n",
    "           'infection_urinary', 'infection_cns', 'infection_ssi', 'csfl_ne', 'csfl_ss'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Adfuller tests to check stationarity: If we fail to reject the null hypothesis (p-value >0.05),\n",
    "##### we can say that the series is non-stationary.\n",
    "\n",
    "# Define function to calculate timeseries statistics\n",
    "def get_yy_by_col(dataframe, yy_col='gender_M', date_col='yearmonth', abs_values='ratio',\n",
    "                  inner_agg_func='max', uid='ID_subid', return_by=None):\n",
    "    assert date_col in ['yearmonth', 'year', 'date', 'halfyear']\n",
    "    assert return_by != 'year' or date_col == 'date', 'Return by year only works with date'\n",
    "    \n",
    "    ID_subid_first_date = dataframe[[uid, date_col]].groupby(uid).min()\n",
    "\n",
    "    if inner_agg_func in 'sum' and abs_values == 'median':\n",
    "        mask = (dataframe[[uid, yy_col]].groupby(uid).agg(inner_agg_func) > 0)\n",
    "        a = dataframe[[uid, yy_col]].groupby(uid).agg(inner_agg_func).join(ID_subid_first_date)[mask.values].groupby(date_col)\n",
    "    else:  # default behaviour\n",
    "        a = dataframe[[uid, yy_col]].groupby(uid).agg(inner_agg_func).join(ID_subid_first_date).groupby(date_col)\n",
    "    \n",
    "    res_sum = None\n",
    "    if abs_values == 'ratio':\n",
    "        res = a.sum() / a.count()\n",
    "        res_sum = a.sum()\n",
    "    elif abs_values in ['count', 'sum', 'median']:\n",
    "        res = a.agg(abs_values)\n",
    "    elif abs_values == 'meanwithoutzeros':\n",
    "        res = a.agg(lambda x: x.replace(0., np.nan).agg(np.nanmedian))\n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "    if inner_agg_func == 'sum' and abs_values == 'median':\n",
    "        if date_col == 'date':\n",
    "            percentiles_25_75 = a.sum().describe().T.iloc[:,[4,6]]\n",
    "            percentiles_25_75.index = [0]\n",
    "        else:\n",
    "            percentiles_25_75 = a.describe().iloc[:,[4,6]]\n",
    "        \n",
    "        cil = percentiles_25_75.iloc[:,[0]]\n",
    "        cir = percentiles_25_75.iloc[:,[1]]\n",
    "        \n",
    "    elif date_col == 'date':\n",
    "        cil, cir = ci(a.sum().sum(), a.count().sum())\n",
    "        cil, cir = pd.DataFrame(cil).T, pd.DataFrame(cir).T\n",
    "\n",
    "    else:\n",
    "        cil, cir = ci(a.sum(), a.count())\n",
    "    \n",
    "    if cir.isnull().all().any() or cil.isnull().all().any():\n",
    "        if date_col == 'date':\n",
    "            percentiles_25_75 = a.median().describe().T.iloc[:,[4,6]]\n",
    "            percentiles_25_75.index = [0]\n",
    "        else:\n",
    "            percentiles_25_75 = a.describe().iloc[:,[4,6]]\n",
    "        cil = percentiles_25_75.iloc[:,[0]]\n",
    "        cir = percentiles_25_75.iloc[:,[1]]\n",
    "        \n",
    "    cil.columns = ['cil']\n",
    "    cir.columns = ['cir']\n",
    "    \n",
    "    vals = [f\"{l:.3f}; {r:.3f}\" for l, r in zip(cil['cil'], cir['cir'])]\n",
    "    cols = cil.index.astype('str') + '_ci'\n",
    "    res_ci = pd.DataFrame(vals, index=cols, columns=[yy_col])\n",
    "\n",
    "    if date_col == 'date' and inner_agg_func == 'sum' and abs_values == 'median':\n",
    "        result_dict = {yy_col: pd.DataFrame(np.median(res.values), columns=[yy_col], index=['overall']), yy_col + '_ci': res_ci}\n",
    "    elif date_col == 'date' and inner_agg_func == 'max' and abs_values == 'median':\n",
    "        # day_in_icu_max, los        \n",
    "        result_dict = {yy_col: pd.DataFrame(res.median().values, columns=[yy_col], index=['overall']), yy_col + '_ci': res_ci}\n",
    "        \n",
    "    elif date_col == 'date':\n",
    "        result_dict = {yy_col: pd.DataFrame(res.median().values, columns=[yy_col], index=['overall']), yy_col + '_ci': res_ci}\n",
    "    else:\n",
    "        result_dict = {yy_col: res, yy_col + '_ci': res_ci}\n",
    "        \n",
    "    if abs_values == 'ratio':\n",
    "        res_sum.index = res_sum.index.astype('str') + '_count'\n",
    "        result_dict[yy_col + '_count'] = res_sum        \n",
    "\n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess dynamics in factors using both linreg and Adfuller tests; monthly data;\n",
    "# Date for each patient is attributed to the month of admission\n",
    "\n",
    "table = {}\n",
    "for col in FACTORS:\n",
    "    UID = 'ID_subid' if col != 'outcome_death' else 'ID'\n",
    "\n",
    "    if col == 'ID_subid':\n",
    "        abs_value_flag = 'count'\n",
    "    elif 'st_' in col and '_len_mean' in col:\n",
    "        abs_value_flag = 'meanwithoutzeros'\n",
    "    else:\n",
    "        abs_value_flag = 'ratio'    \n",
    "\n",
    "    ts = get_yy_by_col(vae_data_main, yy_col=col, date_col='yearmonth', abs_values=abs_value_flag, uid=UID)[col]\n",
    "\n",
    "    if ts.isna().max()[0]:  # if Nan \n",
    "        ts = ts.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    pval = adfuller(ts.values.reshape(-1))[1]\n",
    "    a = linregress(ts.values.reshape(-1), np.arange(len(ts.values.reshape(-1)))).pvalue\n",
    "\n",
    "    ts2: dict = get_yy_by_col(vae_data_main, yy_col=col, date_col='year', abs_values=abs_value_flag, uid=UID)\n",
    "        \n",
    "    if abs_value_flag == 'ratio':\n",
    "        ts2_count = ts2[col + '_count']\n",
    "        \n",
    "    ts2, ts2_ci = ts2[col], ts2[col + '_ci']  # '2011, 2012, ...'\n",
    "        \n",
    "\n",
    "    ts3 = get_yy_by_col(vae_data_main, yy_col=col, date_col='date', abs_values=abs_value_flag, uid=UID)\n",
    "    ts3, ts3_ci = ts3[col], ts3[col + '_ci']  # 'Overall' column\n",
    "\n",
    "    to_concat = [ts2, ts2_ci, ts3, ts3_ci]\n",
    "    if abs_value_flag == 'ratio':\n",
    "        to_concat += [ts2_count]\n",
    "        \n",
    "    table[col] = pd.concat(to_concat).to_dict()[col]\n",
    "    table[col]['adfuller_pvalue'] = np.round(pval, 5)\n",
    "    table[col]['linreg'] = np.round(a, 5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final table\n",
    "table_years = pd.DataFrame.from_dict(table).T\n",
    "\n",
    "# multiple comparisons adjustment in linreg p-value\n",
    "table_years['adj_linreg'] = multitest.multipletests(table_years.linreg, alpha=0.05, method='holm')[1]\n",
    "\n",
    "table_years.to_csv('./table_years.csv', sep='\\t', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
